## 褚博士课程：
- ELMo
  - 一个预训练两层双向LSTM语言模型。
  - 慢
  - 预训练模型效果很好
  - allenNLP
  - **任何需要embedding的地方把w2v、glove都换成ELMo就行**

- BERT
  - 并不是语言模型，目标是预测masked word
  - 基于transform
  -  输入表示,两句话两句话训练
     -  token：词向量 
     -  segment：要么是第0句话，要么是第1句话
     -  position：句子的第几个位置
  - 任务1 ：预测masked单词
  - 任务2 : 看两个单词是不是相邻的句子
  - 分类问题一般用CLS token

- OpenAI GPT-2    **Generative Pre-Training with Transformer Language Models**
  - 是基于transform的decoder模型
  - GPT2比1加了很多数据，差别很小
  - 