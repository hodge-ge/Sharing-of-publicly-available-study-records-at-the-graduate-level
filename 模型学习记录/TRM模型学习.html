<!doctype html>
<html style='font-size:16px !important'>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>TRM模型学习</title><link href='https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
mark .md-meta { color: rgb(0, 0, 0); opacity: 0.3 !important; }


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

html {
    font-size: 16px;
}

body {
    font-family: "Open Sans","Clear Sans","Helvetica Neue",Helvetica,Arial,sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    padding-bottom: .3em;
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
   padding-bottom: .3em;
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}
h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table tr td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}
table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    table,
    pre {
        page-break-inside: avoid;
    }
    pre {
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}


</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node first-line-indent'><p><span>面试题目系列</span></p><p><a href='https://zhuanlan.zhihu.com/p/352002029' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/352002029</a></p><p><a href='https://zhuanlan.zhihu.com/p/149799951' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/149799951</a></p><p><a href='https://www.bilibili.com/video/BV1Di4y1c7Zm' target='_blank' class='url'>https://www.bilibili.com/video/BV1Di4y1c7Zm</a></p><p><span>本记录包含内容：</span></p><ul><li><span>位置编码</span></li><li><span>多头注意力机制</span></li><li><span>残差和layerNomal</span></li><li><span>前馈神经网络</span></li><li><span>TRM面试</span></li></ul><ol start='' ><li><p><span>位置编码</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183029.png" referrerpolicy="no-referrer" alt="image-20210315231609188"></p></li></ol><p><span>看为黑盒子，初步细化</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183050.png" referrerpolicy="no-referrer" alt="image-20210315231654346"></p><p><span>拆解为 编码器-解码器 类似于seq2seq</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183055.png" referrerpolicy="no-referrer" alt="image-20210315231750021"></p><p><span>拆解为6编码6解码的方块图</span></p><p><strong><span>注意：6个编码器是同时训练的，不共享参数，是6个编码器都在训练。不是训练一个然后复制6个。</span></strong></p><p><span>6个encoder 6个decoder分别结构相同，但是参数不同</span></p><p><span>encoder和decoder结构不同</span></p><p><span>模型图：</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183101.png" referrerpolicy="no-referrer" alt="image-20210316102408371"></p><p><span>左侧是编码器，右侧是解码器</span></p><p><span>能看到编码器和解码器的结构是不同的。 解码器多了交互的多头注意力机制。</span></p><p><span>左右分开进行分析：</span></p><ul><li><p><span>编码器</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183110.png" referrerpolicy="no-referrer" alt="image-20210316102733675"></p><ul><li><p><span>输入部分</span></p><p><span>一个是enmedding，另一个是位置编码</span></p><p><span>位置编码的原因：</span></p><p><span>	</span><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183115.png" referrerpolicy="no-referrer" alt="image-20210316102907775"></p><p><span>RNN的参数是一套的，UVW是一套参数，不是好几套。每个时间步都是共享参数的。UVW是同步更新的。是有时序关系的，先处理我，在处理爱，再处理你。 </span></p><p><span>而TRM模型是有批处理能力的，一起处理速度更快，这就忽略了句子的时序性。我在最前面，爱在中间，你在最后面。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183123.png" referrerpolicy="no-referrer" alt="image-20210316134757245"></p><p><span>偶数位置用sin，奇数位置用cos。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183132.png" referrerpolicy="no-referrer" alt="image-20210316103819372"></p><p><span> </span><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183128.png" referrerpolicy="no-referrer" alt="image-20210316134847596"></p><p><span>PE公式体现了绝对位置信息，公式2推理后得到公式3提现相对位置。</span></p><p><span>你的位置，由我、爱线性表示</span></p></li><li><p><span>注意力机制</span></p><ul><li><p><span>基本的注意力机制</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183143.png" referrerpolicy="no-referrer" alt="image-20210316135607892"></p><p><span>用图片举例</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183147.png" referrerpolicy="no-referrer" alt="image-20210316135656516"></p><p><span>关键是QKV三个矩阵，公式中的d</span><sub><span>k</span></sub><span>是先不用管的。</span></p><p><span>婴儿、左上、左下、右上、右下、v</span><sub><span>1-4</span></sub><span>分别是向量</span></p><p><span>婴儿和左上、左下、右上、右下分别点乘，值越大说明相似度越大，越应该注意。例如0.7 0.1 0.1 0.1 再和V相乘，就得到加权和注意力。</span></p><p><span>用文本举例</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183209.png" alt="image-20210316140345458" style="zoom:80%;" /></p><p><span>展开进行计算</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183156.png" referrerpolicy="no-referrer" alt="image-20210316140434873"></p><p><span>爱和我、不、爱、你 分别计算，得到S</span><sub><span>1-4</span></sub><span>的结果之后再softmax归一，得到a</span><sub><span>1-4</span></sub><span>其和为1.</span></p><p><span>之后看箭头，a</span><sub><span>i</span></sub><span>和Value</span><sub><span>i</span></sub><span>乘做和得到注意力分数</span></p></li><li><p><span>TRM中怎么操作</span></p><p><span>只有词向量的情况下怎么获取QKV矩阵</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183223.png" referrerpolicy="no-referrer" alt="image-20210316140933652"></p><p><span>X乘以W</span><sup><span>Q/K/V</span></sup><span>得到Q/K/V</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183228.png" referrerpolicy="no-referrer" alt="image-20210316141225558"></p><p><span>实际中使用矩阵实现并行</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183233.png" referrerpolicy="no-referrer" alt="image-20210316141341569"></p><p><span>W</span><sup><span>QKV</span></sup><span>是一套参数，是单头注意力。实际中会用多套参数，就是多头注意力机制。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183238.png" referrerpolicy="no-referrer" alt="image-20210316141518007"></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183243.png" referrerpolicy="no-referrer" alt="image-20210316141709728"></p><p>&nbsp;</p><p>&nbsp;</p></li></ul></li><li><p><span>前馈神经网络</span></p><p><span>接下来是残差网络和LayNorm</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183259.png" alt="image-20210316142212157" style="zoom:80%;" /></p><p><span>x是词向量，向上走，和位置编码对位相加，得到新的x</span><sub><span>1,2</span></sub><span> 之后进入注意力层，得到输出结果Z</span><sub><span>1,2</span></sub><span> 之后进入残差网络，将Z矩阵和输入X（位置编码之后）矩阵原封不动的拿来对位相加，再做layerNorm再输出。</span></p><p><span>那么什么是残差？</span></p><p><img src="D:\github_data\Sharing-of-publicly-available-study-records-at-the-graduate-level\模型学习记录\TFM模型学习.assets\image-20210316142604500.png" referrerpolicy="no-referrer" alt="image-20210316142604500"></p><p><span>两个weight可以归结为一个函数F(x),输出结果是加号上面的箭头。拿上输入X和F(x)对位相加，再输出。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183341.png" referrerpolicy="no-referrer" alt="image-20210316143203541"></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183325.png" referrerpolicy="no-referrer" alt="image-20210316143336915"></p><p><span>确保了梯度不消失，因为有1的存在。梯度不消失，网络才能提高深度。</span></p><p><span>几乎见不到堆叠RNN的情况，最多是Bi-LSTM双层双向就很难训练了。</span></p><p>&nbsp;</p><p><span>问题</span><strong><span>为什么不用BN batch norm?</span></strong></p><p><span>很少用BN，几乎都用LN，因为BN在NLP任务重效果不好。</span></p><p><span>  BN：</span><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183315.png" referrerpolicy="no-referrer" alt="image-20210316143939894"></p><p><span>BN在于针对整个batch样本在同一维度做处理</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183350.png" referrerpolicy="no-referrer" alt="image-20210316144047340"></p><p><span>每列代表样本，小红、小绿；每行代表特征，体重、身高。</span></p><p><span>BN是对batch里面的所有人的每一个特征做处理，例如所有人的身高、提高。</span></p><p><span>优点：可以解决内部协变量偏移   缓解梯度饱和问题（使用S函数的话），加快收敛</span></p><p><span>缺点：batch_size小的话效果差   BN在RNN中效果比较差</span></p><p>&nbsp;</p><p><span>为什么用LN？为什么LN对一个样本的所有单词缩放可以起到效果。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183356.png" referrerpolicy="no-referrer" alt="image-20210316150208959"></p><p><span>按照BN的思想，将我、今进行处理，并不能代表什么信息，不能像 体重 那样作为特征来缩放。</span></p><p><span>而LN可以，将“我爱中国共产党” 和 “今天天气真不错”每个句子的所有单词去做均值和分差。含义是认为“我爱中国共产党”语义相近。可以理解是做了加权词向量。</span></p><p>&nbsp;</p><p><span>再看前馈神经：</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183400.png" referrerpolicy="no-referrer" alt="image-20210316150558521"></p><p><span>Z</span><sub><span>1,2</span></sub><span>通过feed forward两层全连接，再过一个残差和LN得到编码结果。</span></p><p>&nbsp;</p><p><span>以上就是编码过程。</span></p></li></ul></li><li><p><span>解码器</span></p><p><span>解码器可以分为两部分</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183411.png" referrerpolicy="no-referrer" alt="image-20210316151012818"></p><p><span>第一模块 掩盖的多头注意力机制</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183419.png" referrerpolicy="no-referrer" alt="image-20210316151221422"></p><p><span>为什么mask?</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183454.png" referrerpolicy="no-referrer" alt="image-20210316151309528"></p><p><span>没有mask时</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183435.png" alt="image-20210316151418229" style="zoom:67%;" /></p><p><span>平时做题看着完全的答案，考试的时候没有答案在手边参考，做题就不好。所以需要平时练习的时候就把答案盖住。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183445.png" alt="image-20210316151604846" style="zoom:50%;" /></p><p><span>把you now给的信息叉掉，确保做题和考试状态的一致性。</span></p><p><span>第二模块 交互层</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183505.png" referrerpolicy="no-referrer" alt="image-20210316151717615"></p><p><span>这里是典型的多头注意力机制。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183547.png" referrerpolicy="no-referrer" alt="image-20210316151754852"></p><p><span>所有编码器的输出和每一个解码器去做交互。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183556.png" referrerpolicy="no-referrer" alt="image-20210316151920666"></p><p><span>记住，编码器输出的是K、V矩阵，解码器生成Q矩阵。</span></p><p>&nbsp;</p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183558.png" referrerpolicy="no-referrer" alt="image-20210316152048833"></p><p><span>虚线代表KV矩阵的输出，是和每一个的解码器的交互层的Q做交互。</span></p><p>&nbsp;</p><hr /><p><span>换个角度：</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183609.png" referrerpolicy="no-referrer" alt="image-20210316181600590"></p></li></ul><p><span>上图可以得知，feed forward是全连接层。TRM的核心就是多头注意力和masked多头注意力。</span></p><p><img src="D:\github_data\Sharing-of-publicly-available-study-records-at-the-graduate-level\模型学习记录\TRM模型学习.assets\image-20210316182017418.png" referrerpolicy="no-referrer" alt="image-20210316182017418"></p><p><span>x乘以矩阵得到a向量，具体的，a可以分为qkv。qkv由x分别乘以权重矩阵得到。q代表索引，k代表键,v代表数值。</span></p><p><strong><span>多头注意力机制=self-attention</span></strong></p><hr /><p><span>拆解来看，单头注意力过程</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183620.png" referrerpolicy="no-referrer" alt="image-20210316183501604"></p><p><span>含义是用x1要得到b1要多x3有多大注意。要做的是用x1得到的q1乘以x3得到的k3</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183626.png" referrerpolicy="no-referrer" alt="image-20210316183700377"></p><p><span>这样就能看出来，用x</span><sub><span>1</span></sub><span>生成b</span><sub><span>1,2,3,4</span></sub><span> 需要对x</span><sub><span>1,2,3,4</span></sub><span> 的注意力。a</span><sup><span>^</span></sup><sub><span>&lt;i,j&gt;</span></sub><span> 求和是1，代表xi对xj的关注率。</span></p><p><span>化简得到：</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183633.png" referrerpolicy="no-referrer" alt="image-20210316184025827"></p><p><span>最后用a</span><sup><span>^</span></sup><sub><span>&lt;i,j&gt;</span></sub><span> 乘以对应的v</span><sub><span>j</span></sub><span> 求和得到b</span><sub><span>i</span></sub></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183639.png" referrerpolicy="no-referrer" alt="image-20210316184456041"></p><p><span>目标：获得b1，要对x</span><sub><span>1-4</span></sub><span> 分别给与多大的注意。</span></p><p><span>同理，扩展到多头注意力：b</span><sup><span>11</span></sup><span> 代表第一套由W</span><sub><span>q\k\v</span></sub><span> 得到的q k v参数计算出来的注意力。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183646.png" referrerpolicy="no-referrer" alt="image-20210316184850659"></p><p><span>如果生成多组的a，比如生成a</span><sup><span>11</span></sup><span> a</span><sup><span>12</span></sup><span> 分别对应q</span><sup><span>11/12</span></sup><span> k</span><sup><span>11/12</span></sup><span> v</span><sup><span>11/12 </span></sup><span> 经过计算得到b12</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183653.png" referrerpolicy="no-referrer" alt="image-20210316223800243"></p><p><span>更换一套参数对应q</span><sup><span>12</span></sup><span> k</span><sup><span>12</span></sup><span> v</span><sup><span>12 </span></sup><span> 经过计算得到得到b</span><sup><span>12</span></sup></p><p><span>与上一套参数对应q</span><sup><span>11</span></sup><span> k</span><sup><span>11</span></sup><span> v</span><sup><span>11 </span></sup><span> 经过计算得到得到b</span><sup><span>11</span></sup></p><p><span>经过整合，得到b1</span></p><p>&nbsp;</p><hr /><p><span>换个角度讲</span></p><p><a href='https://www.bilibili.com/video/BV1P4411F77q/?spm_id_from=333.788.recommend_more_video.5' target='_blank' class='url'>https://www.bilibili.com/video/BV1P4411F77q/?spm_id_from=333.788.recommend_more_video.5</a></p><p><span>TRM理解：</span></p><p><span>	</span><span>TEM是2017年谷歌大佬提出的。Bert是TRM模型衍生出来的预训练模型。</span></p><p><span>	</span><span>应用：  培养作家</span></p><ul><li><p><span>上游任务  学习识字</span></p><p><span>训练预训练模型</span></p></li><li><p><span>下游任务   学习写作</span></p><p><span>NLP实际任务，问答、NER</span></p></li></ul><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183659.png" referrerpolicy="no-referrer" alt="image-20210317102227260"></p><p><span>TRM和LSTM区别：</span></p><ul><li><span>LSTM的训练是迭代的，一个接着一个来，当前这个字过万LSTM，下个字才能过。像是for循环。</span></li><li><span>TRM训练是并行的，所有字同时训练，用位置嵌入理解（positional encoding）文字时序。</span></li></ul><p><span>TRM模型：</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183708.jpg" referrerpolicy="no-referrer" alt="img"></p><p><mark><span>关键的点：解码器的输出又落下来作为输入了</span></mark><span> </span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183713.jpg" referrerpolicy="no-referrer" alt="img"><img src="https://github.com/aespresso/a_journey_into_math_of_ml/raw/e081b67d51a8dc74daa55bb0de35de86acdaa536/03_transformer_tutorial_1st_part/imgs/encoder.jpg" referrerpolicy="no-referrer" alt="img"></p><ul><li><p><span>编码器</span></p><ol start='' ><li><p><span>位置编码</span></p><ul><li><p><span>输入的形状：X [句子个数，句子长度]</span></p></li><li><p><span>之后进入embedding模块，得到嵌入。形状[句子个数，句子长度，嵌入维度]</span></p><p><span>数据来源是 字向量表</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183737.png" referrerpolicy="no-referrer" alt="image-20210317135258679"><span>vocab_size是整个词典里面的字</span></p><p><span>embedding_size是用多少个数字表示每个字 常设为300</span></p><p><span>例如 学 一行数字就表示了学的字向量</span></p></li><li><p><span>positional encoding:</span></p><p><span>位置嵌入的维度和字嵌入的维度维度是一致的。位置嵌入向量和字嵌入向量元素相加即可。</span></p><p><span>用sin cos分别计算偶数、奇数位置的位置信息:</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183903.png" referrerpolicy="no-referrer" alt="image-20210318183903849"></p><p><span>可以发现，周期变化随着序号变大，会越来越慢。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183911.png" referrerpolicy="no-referrer" alt="image-20210317144418899"></p><p><span>这就可以让模型依据纹理的变化学习到位置信息。</span></p></li></ul></li><li><p><span>多头注意力机制</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183915.jpg" referrerpolicy="no-referrer" alt="img"></p><p><span>QK</span><sup><span>T</span></sup><span> 点积，计算出两个向量的相似度，也就是</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183924.png" referrerpolicy="no-referrer" alt="image-20210317145839324"></p><p><span>如果两个词的意思相似，那么点积就大。</span></p><p><span>用QK</span><sup><span>T</span></sup><span> 得到的是注意力方形矩阵，每行代表的是当前的字和这句话所有字的关系。主对角线就是这个字和本身的注意度。为了让当前词和本句的词的关联度和为1，形成概率分布，需要进行softmax归一化。需要先除以k的维度，也就是head_size。</span></p><p><span>之后得到已经归一化的注意力机制方形矩阵，需要用V加权。V是字向量经过线性变换。目前还没动过。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183938.png" referrerpolicy="no-referrer" alt="image-20210317163902340"></p><p><span>左边的行代表当前词和本句所有的词之间的相关性，右边矩阵V的每行是当前这个字的数学表达。</span></p><p><span>用第一个字和本句所有字的相关性乘以本句所有字的字向量的第一个维度，把这个字的和句子的相关性进行了计算。计算完毕之后V是没有改变的。</span></p></li><li><p><span>残差网络和LN</span></p><p><span>残差连接的目的是反向求导的时候，能直接到X而不必要过注意力机制块。</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318183947.png" referrerpolicy="no-referrer" alt="image-20210317170027021"></p></li><li><p><span>前馈网络</span></p><p><span>就是线性层</span></p></li></ol><p><span>整体结构：</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318184003.png" referrerpolicy="no-referrer" alt="image-20210318172556906"></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318184019.png" referrerpolicy="no-referrer" alt="image-20210318174202028"></p><p><span>QKV都是输入X经过线性变换得到的矩阵，形状一样。Q和K的转置相乘含义： Q每行是每个词的字向量表示，K的转置的每列是每个字的字向量表示，通过点积也就是矩阵乘法，得到的是每个字之间的相关度。 因为QKV是由经过加上编码矩阵和X的和求线性运算得到的，因此同时也考虑到了位置信息。如果单把位置矩阵拿出来（不加 X）将会得到主对角线最大值的方阵，也就是位置矩阵。</span></p><p><span>经过计算得到了每个字之间的相关度，得到注意力方阵。之后将注意力方阵乘以V得到和V形状一样的隐藏矩阵。相乘的含义是：注意力矩阵是当前词和当前词的所在句所有词的相关度，乘以每个词的词向量表示的第一维度，将信息融入到词向量表示中。</span></p></li><li><p><span>解码器</span></p></li></ul><p><span>第二部分:</span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318184028.png" referrerpolicy="no-referrer" alt="image-20210317215719564"></p><ol start='' ><li><p><span>位置编码 sin cos线性变换函数，两个字越近向关联度越大。这也符合基础假设：越近越相关。</span></p><p><span>由注意力矩阵的得来来说明位置编码能实现序列信息</span></p><p><span>注意力矩阵每个元素是说当前字和所在句的所有词的相关度。位置编码和注意力矩阵类似，也是说的当前字和所在句的所有词的位置相关性。  都是主对角线最高，自己和自己最相关。</span></p></li><li><p><span>Bidirectional Encoder Representations from Transformers, 如果翻译过来也就是</span><strong><span>双向transformer编码表达</span></strong><span> </span></p><p><img src="https://raw.githubusercontent.com/hodge-ge/imgbed/main/20210318184034.png" referrerpolicy="no-referrer" alt="img"></p><p><span>每层的TRM能拿到所有输入的信息，这就是双向。</span></p><p><span>在BERT中两种预训练的方式来建立语言模型：</span></p><ol start='' ><li><p><span>Masked LM</span></p><p><span>就是随机遮盖或者替换，</span></p></li><li><p><span>NSP next sentence prediction</span></p><p><span> </span></p></li></ol></li></ol><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p></div>
</body>
</html>